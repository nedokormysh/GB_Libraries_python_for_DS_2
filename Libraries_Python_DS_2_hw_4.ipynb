{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Libraries_Python_DS_2_hw_4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO3KV90LuTMxBkWJFTP/jk8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nedokormysh/GB_Libraries_python_for_DS_2/blob/main/Libraries_Python_DS_2_hw_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Вопрос 1.\n",
        "\n",
        "**Расскажите, как работает регуляризация в решающих деревьях, какие параметры мы штрафуем в данных алгоритмах?**\n"
      ],
      "metadata": {
        "id": "Gcw3Zd2r5k2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L1-регуляризация - это добавление к функции потерь суммы весов по модулю. Она способствует разреженности функции, когда лишь немногие факторы не равны нулю.\n",
        "\n",
        "L1-регуляризация реализует это путём отбора наиболее важных факторов, которые сильнее всего влияют на результат. \n",
        "\n",
        "L2-регуляризация предотвращает переобучения модели путём запрета на непропорционально большие весовые коэффициенты. Т.е. функция потерь с L2-регуляризацией занижает пики прибавляя сумму весов в квадрате с множителем лямбда.\n",
        "\n",
        "\n",
        "В решающих деревьях, которые легко переобучаются, процесс ветвления надо в какой-то момент останавливать.\n",
        "\n",
        "Для этого есть разные критерии, обычно используются все сразу:\n",
        "\n",
        "* ограничение по максимальной глубине дерева;\n",
        "* ограничение на минимальное количество объектов в листе;\n",
        "* ограничение на максимальное количество листьев в дереве;\n",
        "* требование, чтобы функционал качества Branch при делении текущей подвыборки на две улучшался не менее чем на s процентов.\n",
        "\n",
        "\n",
        "Это делается либо прямо во время построения дерева, либо построить дерево без ограничений, а затем удалить некоторые вершины дерева, без существенной потери качества. "
      ],
      "metadata": {
        "id": "BAabmBfx_F6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Вопрос 2.\n",
        "\n",
        "**По какому принципу рассчитывается \"важность признака (feature_importance)\" в ансамблях деревьев?**"
      ],
      "metadata": {
        "id": "2IlotOtt7xON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если построить много деревьев решений (случайный лес), то чем выше в среднем признак в дереве решений, тем он важнее в данной задаче классификации/регрессии. \n",
        "\n",
        "Признаки, используемые в верхней части дерева, влияют на окончательное предсказание для большей доли обучающих объектов, чем признаки, попавшие на более глубокие уровни. Таким образом, ожидаемая доля обучающих (feature_importance), для которых происходило ветвление по данному признаку, может быть использована в качестве оценки его относительной важности для итогового предсказания. Усредняя полученные оценки важности признаков по всем решающим деревьям из ансамбля, можно уменьшить дисперсию такой оценки и использовать ее для отбора признаков. Этот метод известен как MDI (mean decrease in impurity). Существуют и другие методы оценки важности признаков для ансамблей: например, Permutation feature importance (уменьшение оценки модели при случайном перемешивании одного признка модели. Эта процедура разрывает связь между признаком и целью).\n"
      ],
      "metadata": {
        "id": "Uch1DdefGskR"
      }
    }
  ]
}