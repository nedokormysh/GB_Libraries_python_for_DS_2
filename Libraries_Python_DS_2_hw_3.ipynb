{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Libraries_Python_DS_2_hw_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNPMKiPcGJm3RseW4h4HGXP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nedokormysh/GB_Libraries_python_for_DS_2/blob/main/Libraries_Python_DS_2_hw_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Вопрос 1.\n",
        "\n",
        "Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?"
      ],
      "metadata": {
        "id": "qlCa3EwbC4kK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Макро** — вычисление метрики для каждого класса и получение невзвешенного среднего. Т.е. просто среднее от рассчитанных метрик по классам. Каждому классу придаётся одинаковый вес. Таким образом этот вариант усреднения более чувствителен к ошибке по одному из классов и не подходит для несбалансированных датасетов. Но возможно нам следует использовать, в задачах, когда нам нужно подчеркнуть влияние редкого класса.\n",
        "\n",
        "**Взвешенный** — вычисление метрик для каждого класса и получение взвешенного среднего по числу выборок на каждый класс. (т.е. заложено количество записей по классу). Менее чувствительна к ошибке по одному из классов. Подходит для несбалансированных датасетов.\n",
        "\n",
        "**Микро** — расчет метрики на глобальном уровне путем подсчета итоговых истинных положительных результатов, ложноотрицательных и ложноположительных результатов (независимо от классов). Подходит для несбалансированных датасетов.\n",
        "\n"
      ],
      "metadata": {
        "id": "Nkg4cqtxDam2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Вопрос 2.\n",
        "\n",
        "В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?"
      ],
      "metadata": {
        "id": "0UrcQ1-aDVFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LightGBM** уникален тем, что может строить деревья с использованием односторонней выборки на основе градиента (GOSS).\n",
        "\n",
        "Градиентная односторонняя выборка (GOSS)  анализирует как градиенты различных срезов экземпляров данных влияют на функцию потерь.\n",
        "\n",
        "С помощью GOSS исключается значительная доля экземпляров данных с небольшими градиентами. Поскольку экземпляры данных с большими градиентами играют более важную роль в вычислении информационного выигрыша, GOSS может получить довольно точную оценку информационного выигрыша с гораздо меньшим размером данных.\n",
        "\n",
        "Также используется технология Exclusive Feature Bundling (объединение взаимоисключающих признаков) — это подход объединения разрежённых  взаимоисключающих признаков, таких как категориальные переменные входных данных, закодированные унитарным кодированием. Таким образом, это тип автоматического подбора признаков.\n",
        "\n",
        "LightGBM использует рост дерева по листьям. Он выбирает для роста тот лист, который минимизирует потери, что позволяет вырастить несбалансированное дерево.\n",
        "\n",
        "**CatBoost** отличается от LightGBM и XGBoost тем, что фокусируется на оптимизации деревьев решений для категориальных переменных или переменных, разные значения которых могут не иметь отношения друг к другу.\n",
        "\n",
        "CatBoost автоматически определяет разные категории без необходимости предварительной обработки.\n",
        "\n",
        "Catboost предлагает технику выборки под названием Minimal Variance Sampling (MVS). В этой технике взвешенная выборка происходит на уровне дерева, а не на уровне разбиения. Наблюдения для каждого дерева бустинга отбираются таким образом, чтобы максимизировать точность оценки разбиения.\n",
        "\n",
        "Catboost строит сбалансированное дерево. На каждом уровне такого дерева выбирается пара признак-сплит, которая приносит наименьшие потери (согласно штрафной функции) и используется для всех узлов уровня.\n",
        "\n",
        "**XGBoost** не имеет встроенного метода для категориальных признаков. Кодирование (one-hot, целевое кодирование и т.д.) должно выполняться пользователем.\n",
        "\n",
        "XGboost не использует никаких методов взвешенной выборки, что делает его процесс разбиения более медленным по сравнению с GOSS и MVS.\n",
        "\n",
        "XGboost разделяет до заданного гиперпараметра max_depth. Затем начинает обрезать дерево в обратном направлении и удаляет части, за пределами которых нет положительного выигрыша. XGBoost также может выполнять рост дерева по листьям."
      ],
      "metadata": {
        "id": "K7QA44m20TbL"
      }
    }
  ]
}